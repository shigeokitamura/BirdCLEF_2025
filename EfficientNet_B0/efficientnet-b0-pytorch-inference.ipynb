{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf281966",
   "metadata": {
    "papermill": {
     "duration": 0.004907,
     "end_time": "2025-05-19T22:05:25.793434",
     "exception": false,
     "start_time": "2025-05-19T22:05:25.788527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031d0ad9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:25.802931Z",
     "iopub.status.busy": "2025-05-19T22:05:25.802655Z",
     "iopub.status.idle": "2025-05-19T22:05:40.946445Z",
     "shell.execute_reply": "2025-05-19T22:05:40.945856Z"
    },
    "papermill": {
     "duration": 15.149941,
     "end_time": "2025-05-19T22:05:40.947792",
     "exception": false,
     "start_time": "2025-05-19T22:05:25.797851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518f9ad",
   "metadata": {
    "papermill": {
     "duration": 0.003702,
     "end_time": "2025-05-19T22:05:40.955616",
     "exception": false,
     "start_time": "2025-05-19T22:05:40.951914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1908c2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:40.964498Z",
     "iopub.status.busy": "2025-05-19T22:05:40.964036Z",
     "iopub.status.idle": "2025-05-19T22:05:41.020369Z",
     "shell.execute_reply": "2025-05-19T22:05:41.019716Z"
    },
    "papermill": {
     "duration": 0.062194,
     "end_time": "2025-05-19T22:05:41.021570",
     "exception": false,
     "start_time": "2025-05-19T22:05:40.959376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Data Paths\n",
    "    test_soundscapes: str = \"/kaggle/input/birdclef-2025/test_soundscapes\"\n",
    "    submission_csv: str = \"/kaggle/input/birdclef-2025/sample_submission.csv\"\n",
    "    taxonomy_csv: str = \"/kaggle/input/birdclef-2025/taxonomy.csv\"\n",
    "    model_path: str = \"/kaggle/input/efficientnet-b0-pytorch-train\"\n",
    "\n",
    "    # Audio parameters\n",
    "    # Sampling rate for audio processing (samples per second).\n",
    "    FS: int = 32000\n",
    "    # The duration of each audio segment to be processed in seconds.\n",
    "    WINDOW_SIZE: int = 5\n",
    "\n",
    "    # Mel spectrogram parameters\n",
    "    # The number of FFT components used to compute the spectrogram.\n",
    "    N_FFT: int = 1024\n",
    "    # The number of samples between successive frames in the spectrogram.\n",
    "    HOP_LENGTH: int = 512\n",
    "    # The number of Mel bands to generate.\n",
    "    N_MELS: int = 128\n",
    "    # The minimum frequency (in Hz) to include in the Mel spectrogram.\n",
    "    FMIN: int = 50\n",
    "    # The maximum frequency (in Hz) to include in the Mel spectrogram.\n",
    "    FMAX: int = 14000\n",
    "    # The target shape (height, width) for the mel spectrogram image after resizing.\n",
    "    TARGET_SHAPE: tuple[int, int] = (256, 256)\n",
    "\n",
    "    # Model parameters\n",
    "    # The name of the base model architecture to use (e.g., \"efficientnet_b0\").\n",
    "    model_name: str = \"efficientnet_b0\"\n",
    "    # The number of input channels for the model (1 for grayscale/mel spectrogram).\n",
    "    in_channels: int = 1\n",
    "    # The device to use for inference (\"cuda\" for GPU, \"cpu\" for CPU).\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Inference parameters\n",
    "    # The number of audio segments to process in a single batch during inference.\n",
    "    batch_size: int = 16\n",
    "    # Flag to enable or disable Test-Time Augmentation (TTA).\n",
    "    use_tta: bool = False\n",
    "    # The number of TTA variations to apply if use_tta is True.\n",
    "    tta_count: int = 3\n",
    "    # The probability threshold to consider a species as detected in a segment.\n",
    "    threshold: int = 0.5\n",
    "\n",
    "    # Model selection\n",
    "    # Flag to use only models from specific folds if True. If False, all found models are used.\n",
    "    use_specific_folds: bool = False\n",
    "    # A list of fold numbers to use if use_specific_folds is True.\n",
    "    folds: tuple[int, int] = [0, 1]  # Used only if use_specific_folds is True\n",
    "\n",
    "    # Debugging\n",
    "    # Flag to enable debug mode. If True, only a small subset of test files is processed.\n",
    "    debug: bool = False\n",
    "    # The number of test files to process in debug mode.\n",
    "    debug_count: bool = 3\n",
    "\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c9418e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.030212Z",
     "iopub.status.busy": "2025-05-19T22:05:41.029920Z",
     "iopub.status.idle": "2025-05-19T22:05:41.054769Z",
     "shell.execute_reply": "2025-05-19T22:05:41.053915Z"
    },
    "papermill": {
     "duration": 0.030329,
     "end_time": "2025-05-19T22:05:41.055889",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.025560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df[\"primary_label\"].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d816cf68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.064554Z",
     "iopub.status.busy": "2025-05-19T22:05:41.064337Z",
     "iopub.status.idle": "2025-05-19T22:05:41.071202Z",
     "shell.execute_reply": "2025-05-19T22:05:41.070621Z"
    },
    "papermill": {
     "duration": 0.012376,
     "end_time": "2025-05-19T22:05:41.072184",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.059808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch model for bird song classification using a backbone architecture\n",
    "    (like EfficientNet) followed by a classifier.\n",
    "\n",
    "    Args:\n",
    "        cfg (CFG): Configuration object containing model and data parameters.\n",
    "        num_classes (int): The number of output classes (bird species).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: CFG, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,  # Set to False as we are loading weights later\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "\n",
    "        # Modify the classifier layer of the backbone based on its type\n",
    "        if \"efficientnet\" in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif \"resnet\" in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            # For other timm models, get classifier features and reset it\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, \"\")\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (mel spectrogram), expected shape\n",
    "                              (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits from the classifier, shape (batch_size, num_classes).\n",
    "        \"\"\"\n",
    "\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Handle potential dictionary output from some backbones\n",
    "        if isinstance(features, dict):\n",
    "            features = features[\"features\"]\n",
    "\n",
    "        # Apply pooling if the features are 4D (image-like)\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)  # Flatten the features\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc28ad",
   "metadata": {
    "papermill": {
     "duration": 0.003552,
     "end_time": "2025-05-19T22:05:41.079532",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.075980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Audio Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c005154e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.087810Z",
     "iopub.status.busy": "2025-05-19T22:05:41.087565Z",
     "iopub.status.idle": "2025-05-19T22:05:41.092523Z",
     "shell.execute_reply": "2025-05-19T22:05:41.091968Z"
    },
    "papermill": {
     "duration": 0.010385,
     "end_time": "2025-05-19T22:05:41.093594",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.083209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data: np.ndarray, cfg: CFG) -> np.ndarray:\n",
    "    \"\"\"Convert audio data to mel spectrogram\n",
    "\n",
    "    Args:\n",
    "        audio_data (np.ndarray): The input audio data as a NumPy array.\n",
    "        cfg (CFG): Configuration object containing mel spectrogram parameters\n",
    "             like FS, N_FFT, HOP_LENGTH, N_MELS, FMIN, FMAX.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The normalized mel spectrogram as a NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle potential NaN values in the audio data\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    # Compute the mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,  # Use power=2.0 for the power spectrogram\n",
    "    )\n",
    "\n",
    "    # Convert power spectrogram to decibels (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize the dB mel spectrogram to a 0-1 range\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (\n",
    "        mel_spec_db.max() - mel_spec_db.min() + 1e-8\n",
    "    )\n",
    "\n",
    "    return mel_spec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9419f732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.102434Z",
     "iopub.status.busy": "2025-05-19T22:05:41.101997Z",
     "iopub.status.idle": "2025-05-19T22:05:41.106809Z",
     "shell.execute_reply": "2025-05-19T22:05:41.106140Z"
    },
    "papermill": {
     "duration": 0.010445,
     "end_time": "2025-05-19T22:05:41.107912",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.097467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_audio_segment(audio_data: np.ndarray, cfg: CFG) -> np.ndarray:\n",
    "    \"\"\"Process audio segment to get mel spectrogram\n",
    "\n",
    "    Pads the audio segment if it's shorter than the window size, converts it\n",
    "    to a mel spectrogram using audio2melspec, and resizes the spectrogram\n",
    "    to the target shape specified in the config.\n",
    "\n",
    "    Args:\n",
    "        audio_data (np.ndarray): The input audio data segment as a NumPy array.\n",
    "        cfg (CFG): Configuration object containing audio and mel spectrogram parameters\n",
    "             like FS, WINDOW_SIZE, TARGET_SHAPE.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The processed and resized mel spectrogram as a NumPy array\n",
    "                    of type np.float32.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad the audio data if its length is less than the required window size\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(\n",
    "            audio_data, (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), mode=\"constant\"\n",
    "        )\n",
    "\n",
    "    # Convert the audio data segment to a mel spectrogram\n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "\n",
    "    # Resize the mel spectrogram if its shape does not match the target shape\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(\n",
    "            mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "\n",
    "    # Return the processed mel spectrogram, ensuring it's of type float32\n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c38dcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.116480Z",
     "iopub.status.busy": "2025-05-19T22:05:41.115927Z",
     "iopub.status.idle": "2025-05-19T22:05:41.119603Z",
     "shell.execute_reply": "2025-05-19T22:05:41.119079Z"
    },
    "papermill": {
     "duration": 0.008832,
     "end_time": "2025-05-19T22:05:41.120579",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.111747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg: CFG) -> list[str]:\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "\n",
    "    Args:\n",
    "        cfg: Configuration object containing audio and mel spectrogram parameters\n",
    "             like FS, WINDOW_SIZE, TARGET_SHAPE.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Paths of model files\n",
    "    \"\"\"\n",
    "\n",
    "    model_files = []\n",
    "\n",
    "    model_dir = Path(cfg.model_path)\n",
    "\n",
    "    for path in model_dir.glob(\"**/*.pth\"):\n",
    "        model_files.append(str(path))\n",
    "\n",
    "    return model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5072de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.129181Z",
     "iopub.status.busy": "2025-05-19T22:05:41.128683Z",
     "iopub.status.idle": "2025-05-19T22:05:41.134594Z",
     "shell.execute_reply": "2025-05-19T22:05:41.134089Z"
    },
    "papermill": {
     "duration": 0.01121,
     "end_time": "2025-05-19T22:05:41.135579",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.124369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_models(cfg: CFG, num_classes: int) -> list[BirdCLEFModel]:\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "\n",
    "    Searches for .pth model files in the directory specified by cfg.model_path.\n",
    "    If use_specific_folds is True in the config, it filters the models to\n",
    "    include only those from the specified folds. Each found model file is\n",
    "    loaded into a BirdCLEFModel instance, moved to the configured device,\n",
    "    and set to evaluation mode.\n",
    "\n",
    "    Args:\n",
    "        cfg (cfg): Configuration object containing model loading parameters\n",
    "             like model_path, use_specific_folds, folds, device.\n",
    "        num_classes (int): The number of output classes for the models.\n",
    "\n",
    "    Returns:\n",
    "        list[BirdCLEFModel]: A list of loaded BirdCLEFModel instances. Returns an empty list\n",
    "              if no models are found or loaded successfully.\n",
    "    \"\"\"\n",
    "\n",
    "    models = []\n",
    "\n",
    "    model_files = find_model_files(cfg)\n",
    "\n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "\n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "\n",
    "    if cfg.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in cfg.folds:\n",
    "            # Basic check for fold number in file path string\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(\n",
    "            f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\"\n",
    "        )\n",
    "\n",
    "    # Load models individually\n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            # Load the model checkpoint, mapping to the specified device\n",
    "            checkpoint = torch.load(\n",
    "                model_path, map_location=torch.device(cfg.device), weights_only=False\n",
    "            )\n",
    "\n",
    "            # Initialize the model architecture\n",
    "            model = BirdCLEFModel(cfg, num_classes)\n",
    "\n",
    "            # Load the state dictionary from the checkpoint\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "            # Move the model to the specified device\n",
    "            model = model.to(cfg.device)\n",
    "\n",
    "            # Set the model to evaluation mode (disables dropout, batch normalization updates, etc.)\n",
    "            model.eval()\n",
    "\n",
    "            # Add the loaded model to the list\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            # Print an error message if loading fails for a specific model\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6fe09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.144324Z",
     "iopub.status.busy": "2025-05-19T22:05:41.144072Z",
     "iopub.status.idle": "2025-05-19T22:05:41.154471Z",
     "shell.execute_reply": "2025-05-19T22:05:41.153934Z"
    },
    "papermill": {
     "duration": 0.015974,
     "end_time": "2025-05-19T22:05:41.155509",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.139535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_spectrogram(\n",
    "    audio_path: str, models: list[BirdCLEFModel], cfg: CFG, species_ids\n",
    ") -> tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process a single audio file and predict species presence for each 5-second segment.\n",
    "\n",
    "    Loads an audio file, divides it into segments, processes each segment\n",
    "    into a mel spectrogram, applies TTA if enabled, passes the spectrogram(s)\n",
    "    through the model(s), and collects the predicted probabilities for each\n",
    "    species in each segment.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): The file path to the audio file (e.g., .ogg).\n",
    "        models (list[BirdCLEFModel]): A list of loaded BirdCLEFModel instances for inference\n",
    "                (can be a single model in a list or an ensemble).\n",
    "        cfg (CFG): Configuration object containing inference parameters\n",
    "             like FS, WINDOW_SIZE, use_tta, tta_count, device.\n",
    "        species_ids (list[int]): A list of species IDs corresponding to the model output indices.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - list: A list of row_ids (strings) for each segment.\n",
    "            - list: A list of NumPy arrays, where each array contains the\n",
    "                    predicted probabilities for all species for a segment.\n",
    "              Returns empty lists within the tuple if processing fails.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        # Load the full audio file\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "\n",
    "        # Calculate the total number of full 5-second segments\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        # Process each segment\n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "\n",
    "            # Determine the end time for the row_id\n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            # Inference with or without TTA\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "\n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    # Process segment and apply TTA\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "                    # Prepare spectrogram for the model (add batch and channel dimensions)\n",
    "                    mel_spec = (\n",
    "                        torch.tensor(mel_spec, dtype=torch.float32)\n",
    "                        .unsqueeze(0)\n",
    "                        .unsqueeze(0)\n",
    "                    )\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    # Perform inference with single model or ensemble for this TTA variation\n",
    "                    if len(models) == 1:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:  # Ensemble\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "\n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)\n",
    "\n",
    "                # Average predictions across all TTA variations\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:  # No TTA\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "\n",
    "                # Prepare spectrogram for the model\n",
    "                mel_spec = (\n",
    "                    torch.tensor(mel_spec, dtype=torch.float32)\n",
    "                    .unsqueeze(0)\n",
    "                    .unsqueeze(0)\n",
    "                )\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                # Perform inference with single model or ensemble\n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:  # Ensemble\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "\n",
    "            predictions.append(final_preds)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print an error if processing the audio file fails\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bacc1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.164777Z",
     "iopub.status.busy": "2025-05-19T22:05:41.164148Z",
     "iopub.status.idle": "2025-05-19T22:05:41.168580Z",
     "shell.execute_reply": "2025-05-19T22:05:41.168018Z"
    },
    "papermill": {
     "duration": 0.010313,
     "end_time": "2025-05-19T22:05:41.169664",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.159351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec: np.ndarray, tta_idx: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply test-time augmentation to a mel spectrogram.\n",
    "\n",
    "    Applies different transformations to the input spectrogram based on the\n",
    "    augmentation index. Supported transformations are original, horizontal flip\n",
    "    (time shift), and vertical flip (frequency shift).\n",
    "\n",
    "    Args:\n",
    "        spec (np.ndarray): The input mel spectrogram as a NumPy array.\n",
    "        tta_idx (int): The index specifying which TTA transformation to apply.\n",
    "                       - 0: Original (no transformation)\n",
    "                       - 1: Horizontal flip (time axis)\n",
    "                       - 2: Vertical flip (frequency axis)\n",
    "                       - Others: Original (no transformation)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The augmented mel spectrogram as a NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip) by flipping along the second axis (columns)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip) by flipping along the first axis (rows)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        # Default to original if index is not recognized\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76726650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.178587Z",
     "iopub.status.busy": "2025-05-19T22:05:41.177988Z",
     "iopub.status.idle": "2025-05-19T22:05:41.183706Z",
     "shell.execute_reply": "2025-05-19T22:05:41.183008Z"
    },
    "papermill": {
     "duration": 0.011406,
     "end_time": "2025-05-19T22:05:41.184942",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.173536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    cfg: CFG, models: list[BirdCLEFModel], species_ids: list[int]\n",
    ") -> tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run inference on all test soundscapes.\n",
    "\n",
    "    Finds all audio files in the test soundscape directory, optionally limits\n",
    "    the number of files in debug mode, and calls predict_on_spectrogram for\n",
    "    each audio file to get predictions for all segments. Aggregates the\n",
    "    results from all files.\n",
    "\n",
    "    Args:\n",
    "        cfg: Configuration object containing inference parameters\n",
    "             like test_soundscapes, debug, debug_count.\n",
    "        models: A list of loaded BirdCLEFModel instances for inference.\n",
    "        species_ids: A list of species IDs used for predictions.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - list: A list of all row_ids from all processed segments\n",
    "                    across all test files.\n",
    "            - list: A list of NumPy arrays, where each array contains the\n",
    "                    predicted probabilities for all species for a segment.\n",
    "                    This list contains results from all segments of all files.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find all audio files in the test soundscapes directory\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob(\"*.ogg\"))\n",
    "\n",
    "    # Apply debug mode if enabled\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[: cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Process each test audio file using tqdm for a progress bar\n",
    "    for audio_path in tqdm(test_files):\n",
    "        # Get predictions for all segments of the current audio file\n",
    "        row_ids, predictions = predict_on_spectrogram(\n",
    "            str(audio_path), models, cfg, species_ids\n",
    "        )\n",
    "\n",
    "        # Extend the master lists with results from the current file\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Return the aggregated row IDs and predictions\n",
    "    return all_row_ids, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ae2eafb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.193766Z",
     "iopub.status.busy": "2025-05-19T22:05:41.193545Z",
     "iopub.status.idle": "2025-05-19T22:05:41.199552Z",
     "shell.execute_reply": "2025-05-19T22:05:41.198838Z"
    },
    "papermill": {
     "duration": 0.011831,
     "end_time": "2025-05-19T22:05:41.200610",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.188779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_submission(\n",
    "    row_ids: list[int], predictions: list[np.ndarray], species_ids: list[int], cfg: CFG\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create submission dataframe in the required format.\n",
    "\n",
    "    Constructs a pandas DataFrame from the row IDs and predictions.\n",
    "    It ensures that all species columns present in the sample submission file\n",
    "    are included, adding columns with 0.0 values for any missing species.\n",
    "    The columns are ordered according to the sample submission.\n",
    "\n",
    "    Args:\n",
    "        row_ids (list): A list of row_ids for each prediction segment.\n",
    "        predictions (list): A list of NumPy arrays, where each array contains the\n",
    "                             predicted probabilities for all species for a segment.\n",
    "        species_ids (list): A list of species IDs corresponding to the order of\n",
    "                            probabilities in the predictions arrays.\n",
    "        cfg: Configuration object containing the path to the sample submission CSV\n",
    "             (submission_csv).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame formatted for submission, with 'row_id'\n",
    "                      as the first column and species columns following.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    # Create a dictionary to build the DataFrame\n",
    "    submission_dict = {\"row_id\": row_ids}\n",
    "\n",
    "    # Add columns for each species with their predicted probabilities\n",
    "    # Each 'pred' in predictions is a numpy array of probabilities corresponding to species_ids order\n",
    "    for i, species in enumerate(species_ids):\n",
    "        # Collect the prediction probability for the i-th species across all segments\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    # Create the initial DataFrame\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    # Set \"row_id\" as the index temporarily for alignment with sample submission\n",
    "    submission_df.set_index(\"row_id\", inplace=True)\n",
    "\n",
    "    # Read the sample submission to get the required columns and order\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col=\"row_id\")\n",
    "\n",
    "    # Check for any species columns present in the sample submission but missing in our dataframe\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        # Add missing columns with default value 0.0\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    # Reindex the submission dataframe to match the column order of the sample submission\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    # Reset the index to make 'row_id' a regular column again\n",
    "    submission_df = submission_df.reset_index()\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf614c0",
   "metadata": {
    "papermill": {
     "duration": 0.003668,
     "end_time": "2025-05-19T22:05:41.208145",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.204477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4444e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.216901Z",
     "iopub.status.busy": "2025-05-19T22:05:41.216291Z",
     "iopub.status.idle": "2025-05-19T22:05:41.220446Z",
     "shell.execute_reply": "2025-05-19T22:05:41.219733Z"
    },
    "papermill": {
     "duration": 0.009739,
     "end_time": "2025-05-19T22:05:41.221655",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.211916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting BirdCLEF-2025 inference...\")\n",
    "print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39433d67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:41.230290Z",
     "iopub.status.busy": "2025-05-19T22:05:41.230058Z",
     "iopub.status.idle": "2025-05-19T22:05:47.118053Z",
     "shell.execute_reply": "2025-05-19T22:05:47.117234Z"
    },
    "papermill": {
     "duration": 5.894632,
     "end_time": "2025-05-19T22:05:47.120188",
     "exception": false,
     "start_time": "2025-05-19T22:05:41.225556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a total of 5 model files.\n",
      "Loading model: /kaggle/input/efficientnet-b0-pytorch-train/model_fold0.pth\n",
      "Loading model: /kaggle/input/efficientnet-b0-pytorch-train/model_fold3.pth\n",
      "Loading model: /kaggle/input/efficientnet-b0-pytorch-train/model_fold1.pth\n",
      "Loading model: /kaggle/input/efficientnet-b0-pytorch-train/model_fold2.pth\n",
      "Loading model: /kaggle/input/efficientnet-b0-pytorch-train/model_fold4.pth\n"
     ]
    }
   ],
   "source": [
    "models = load_models(cfg, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e205e581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.140161Z",
     "iopub.status.busy": "2025-05-19T22:05:47.139870Z",
     "iopub.status.idle": "2025-05-19T22:05:47.143889Z",
     "shell.execute_reply": "2025-05-19T22:05:47.143129Z"
    },
    "papermill": {
     "duration": 0.014494,
     "end_time": "2025-05-19T22:05:47.145389",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.130895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not models:\n",
    "    raise Exception(\"No models found! Please check model paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d7f3d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.159357Z",
     "iopub.status.busy": "2025-05-19T22:05:47.158637Z",
     "iopub.status.idle": "2025-05-19T22:05:47.163019Z",
     "shell.execute_reply": "2025-05-19T22:05:47.162337Z"
    },
    "papermill": {
     "duration": 0.013037,
     "end_time": "2025-05-19T22:05:47.164618",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.151581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model usage: Ensemble of 5 models\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2693d536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.176819Z",
     "iopub.status.busy": "2025-05-19T22:05:47.176251Z",
     "iopub.status.idle": "2025-05-19T22:05:47.185225Z",
     "shell.execute_reply": "2025-05-19T22:05:47.184619Z"
    },
    "papermill": {
     "duration": 0.014934,
     "end_time": "2025-05-19T22:05:47.186256",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.171322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/kaggle/input/birdclef-2025/test_soundscapes/readme.txt')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Path(cfg.test_soundscapes).glob(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2650bbf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.195143Z",
     "iopub.status.busy": "2025-05-19T22:05:47.194936Z",
     "iopub.status.idle": "2025-05-19T22:05:47.210175Z",
     "shell.execute_reply": "2025-05-19T22:05:47.209411Z"
    },
    "papermill": {
     "duration": 0.022201,
     "end_time": "2025-05-19T22:05:47.212617",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.190416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de3c52f55a04d378de9cc08e09f13ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_ids, predictions = run_inference(cfg, models, species_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1e82bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.222364Z",
     "iopub.status.busy": "2025-05-19T22:05:47.221862Z",
     "iopub.status.idle": "2025-05-19T22:05:47.250404Z",
     "shell.execute_reply": "2025-05-19T22:05:47.249550Z"
    },
    "papermill": {
     "duration": 0.034728,
     "end_time": "2025-05-19T22:05:47.251622",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.216894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n"
     ]
    }
   ],
   "source": [
    "submission_df = create_submission(row_ids, predictions, species_ids, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f01e91a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.261803Z",
     "iopub.status.busy": "2025-05-19T22:05:47.261398Z",
     "iopub.status.idle": "2025-05-19T22:05:47.280444Z",
     "shell.execute_reply": "2025-05-19T22:05:47.279746Z"
    },
    "papermill": {
     "duration": 0.025229,
     "end_time": "2025-05-19T22:05:47.281661",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.256432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data is private except when submitted, so the DataFrame is empty.\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5418bc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T22:05:47.292200Z",
     "iopub.status.busy": "2025-05-19T22:05:47.291930Z",
     "iopub.status.idle": "2025-05-19T22:05:47.299566Z",
     "shell.execute_reply": "2025-05-19T22:05:47.298911Z"
    },
    "papermill": {
     "duration": 0.013866,
     "end_time": "2025-05-19T22:05:47.300622",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.286756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6317e7",
   "metadata": {
    "papermill": {
     "duration": 0.004342,
     "end_time": "2025-05-19T22:05:47.309675",
     "exception": false,
     "start_time": "2025-05-19T22:05:47.305333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "sourceId": 240685063,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.287084,
   "end_time": "2025-05-19T22:05:50.882408",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-19T22:05:21.595324",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1b04eb77c6464053a2c9a107e6858ab2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "1b67eaed47b445be9d8ce182dc63a786": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d87260be63e454ebb3dcf5a7ae512a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_74a90e30d76c48159667fa2d7151af0f",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_86dbbd92a4d54cb4a342a662d2cb0a3f",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "3de3c52f55a04d378de9cc08e09f13ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3d87260be63e454ebb3dcf5a7ae512a5",
        "IPY_MODEL_cee73606966a4768a484775050188827",
        "IPY_MODEL_86e27ec0ccbf4df99f49b832adbbaeac"
       ],
       "layout": "IPY_MODEL_7ad17857a7c245d4acdd482a9d9bfe42",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4974ab3cdfb646f490092801acdd1bfb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5cb9f63768484fa19ff0a0b312b95f95": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "74a90e30d76c48159667fa2d7151af0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ad17857a7c245d4acdd482a9d9bfe42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86dbbd92a4d54cb4a342a662d2cb0a3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86e27ec0ccbf4df99f49b832adbbaeac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b67eaed47b445be9d8ce182dc63a786",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_4974ab3cdfb646f490092801acdd1bfb",
       "tabbable": null,
       "tooltip": null,
       "value": "â€‡0/0â€‡[00:00&lt;?,â€‡?it/s]"
      }
     },
     "cee73606966a4768a484775050188827": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b04eb77c6464053a2c9a107e6858ab2",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5cb9f63768484fa19ff0a0b312b95f95",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
