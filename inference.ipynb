{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd4b1af",
   "metadata": {
    "papermill": {
     "duration": 0.006284,
     "end_time": "2025-06-05T22:18:10.015104",
     "exception": false,
     "start_time": "2025-06-05T22:18:10.008820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üê¶ BirdCLEF 2025: Weighted Blend Inference (Public: 0.879, Private: 0.894)\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "This notebook achieves a public leaderboard score of **0.879** and private score of **0.894** using a **Weighted Blend** approach.\n",
    "\n",
    "## üß© Models Used\n",
    "\n",
    "The final prediction is based on the ensemble of the following public models:\n",
    "\n",
    "- üìò **[Bird2025 | Single SED Model Inference [LB 0.857]](https://www.kaggle.com/code/i2nfinit3y/bird2025-single-sed-model-inference-lb-0-857)**  \n",
    "  by [I2nfinit3y](https://www.kaggle.com/i2nfinit3y)  \n",
    "  A strong single SED model serving as the base for one of the ensemble components.\n",
    "\n",
    "- üß™ **[Post-Processing with Power Adjustment for Low-Rank](https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank)**  \n",
    "  by [MYSO](https://www.kaggle.com/myso1987)  \n",
    "  This notebook introduced a clever post-processing technique to boost low-confidence predictions via power transformation.\n",
    "\n",
    "- üîó **[Bird25 | WeightedBlend | nfnet + convnextv2 | LB.860](https://www.kaggle.com/code/hideyukizushi/bird25-weightedblend-nfnet-convnextv2-lb-860)**  \n",
    "  by [yukiZ](https://www.kaggle.com/hideyukizushi)  \n",
    "  Provided the core blending logic used to combine model outputs.\n",
    "\n",
    "## ‚öñÔ∏è Weighted Blend Strategy\n",
    "\n",
    "We use the weighted average of the two model outputs as follows:\n",
    "\n",
    "- **nfnet**: 50%\n",
    "- **seresnext**: 25%\n",
    "- **efficientnet_v2**: 25%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabecfe",
   "metadata": {
    "papermill": {
     "duration": 0.004699,
     "end_time": "2025-06-05T22:18:10.024992",
     "exception": false,
     "start_time": "2025-06-05T22:18:10.020293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "„Ää„Ää„ÄäSubmission1(nfnet)„Äã„Äã„Äã\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10f12de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:10.036362Z",
     "iopub.status.busy": "2025-06-05T22:18:10.035787Z",
     "iopub.status.idle": "2025-06-05T22:18:28.263764Z",
     "shell.execute_reply": "2025-06-05T22:18:28.262635Z"
    },
    "papermill": {
     "duration": 18.235242,
     "end_time": "2025-06-05T22:18:28.265194",
     "exception": false,
     "start_time": "2025-06-05T22:18:10.029952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "from typing import Union\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dd88d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.277152Z",
     "iopub.status.busy": "2025-06-05T22:18:28.276744Z",
     "iopub.status.idle": "2025-06-05T22:18:28.282246Z",
     "shell.execute_reply": "2025-06-05T22:18:28.281080Z"
    },
    "papermill": {
     "duration": 0.012959,
     "end_time": "2025-06-05T22:18:28.283668",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.270709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3ab668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.295364Z",
     "iopub.status.busy": "2025-06-05T22:18:28.295056Z",
     "iopub.status.idle": "2025-06-05T22:18:28.417031Z",
     "shell.execute_reply": "2025-06-05T22:18:28.415864Z"
    },
    "papermill": {
     "duration": 0.129163,
     "end_time": "2025-06-05T22:18:28.418246",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.289083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: True\n",
      "Number of test soundscapes: 8\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "\n",
    "debug = False\n",
    "if len(file_list) == 0:\n",
    "    debug = True\n",
    "    debug_st_num = 5\n",
    "    debug_num = 8\n",
    "    test_audio_dir = '../input/birdclef-2025/train_soundscapes/'\n",
    "    file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "    file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "    file_list = file_list[debug_st_num:debug_st_num+debug_num]\n",
    "\n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51423eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.430345Z",
     "iopub.status.busy": "2025-06-05T22:18:28.430029Z",
     "iopub.status.idle": "2025-06-05T22:18:28.486475Z",
     "shell.execute_reply": "2025-06-05T22:18:28.485685Z"
    },
    "papermill": {
     "duration": 0.064301,
     "end_time": "2025-06-05T22:18:28.488053",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.423752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=40\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a80da843",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.499867Z",
     "iopub.status.busy": "2025-06-05T22:18:28.499565Z",
     "iopub.status.idle": "2025-06-05T22:18:28.514914Z",
     "shell.execute_reply": "2025-06-05T22:18:28.514256Z"
    },
    "papermill": {
     "duration": 0.022775,
     "end_time": "2025-06-05T22:18:28.516299",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.493524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa3cfd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.528095Z",
     "iopub.status.busy": "2025-06-05T22:18:28.527811Z",
     "iopub.status.idle": "2025-06-05T22:18:28.534246Z",
     "shell.execute_reply": "2025-06-05T22:18:28.533373Z"
    },
    "papermill": {
     "duration": 0.014033,
     "end_time": "2025-06-05T22:18:28.535775",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.521742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/birdclef-2025-sed-models-p/sed0.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed1.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed2.pth']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133b082c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:28.547806Z",
     "iopub.status.busy": "2025-06-05T22:18:28.547537Z",
     "iopub.status.idle": "2025-06-05T22:18:32.670790Z",
     "shell.execute_reply": "2025-06-05T22:18:32.669880Z"
    },
    "papermill": {
     "duration": 4.130874,
     "end_time": "2025-06-05T22:18:32.672224",
     "exception": false,
     "start_time": "2025-06-05T22:18:28.541350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7cf1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:32.685948Z",
     "iopub.status.busy": "2025-06-05T22:18:32.685661Z",
     "iopub.status.idle": "2025-06-05T22:18:32.693293Z",
     "shell.execute_reply": "2025-06-05T22:18:32.691996Z"
    },
    "papermill": {
     "duration": 0.015909,
     "end_time": "2025-06-05T22:18:32.695181",
     "exception": false,
     "start_time": "2025-06-05T22:18:32.679272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1cd65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:32.707331Z",
     "iopub.status.busy": "2025-06-05T22:18:32.707024Z",
     "iopub.status.idle": "2025-06-05T22:18:50.030672Z",
     "shell.execute_reply": "2025-06-05T22:18:50.029602Z"
    },
    "papermill": {
     "duration": 17.331934,
     "end_time": "2025-06-05T22:18:50.032791",
     "exception": false,
     "start_time": "2025-06-05T22:18:32.700857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.25626376271248\n"
     ]
    }
   ],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n",
    "if debug == True:\n",
    "    print(700*(end_t - start)/60/debug_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc0292f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:50.044402Z",
     "iopub.status.busy": "2025-06-05T22:18:50.044125Z",
     "iopub.status.idle": "2025-06-05T22:18:50.067456Z",
     "shell.execute_reply": "2025-06-05T22:18:50.066460Z"
    },
    "papermill": {
     "duration": 0.0307,
     "end_time": "2025-06-05T22:18:50.069032",
     "exception": false,
     "start_time": "2025-06-05T22:18:50.038332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2a13270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:50.080826Z",
     "iopub.status.busy": "2025-06-05T22:18:50.080524Z",
     "iopub.status.idle": "2025-06-05T22:18:50.530095Z",
     "shell.execute_reply": "2025-06-05T22:18:50.529059Z"
    },
    "papermill": {
     "duration": 0.45691,
     "end_time": "2025-06-05T22:18:50.531475",
     "exception": false,
     "start_time": "2025-06-05T22:18:50.074565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n",
      "/tmp/ipykernel_13/3161594100.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_group[cols] = new_predictions\n"
     ]
    }
   ],
   "source": [
    "results.to_csv(\"submission1.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission1.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76045a9",
   "metadata": {
    "papermill": {
     "duration": 0.004959,
     "end_time": "2025-06-05T22:18:50.541894",
     "exception": false,
     "start_time": "2025-06-05T22:18:50.536935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "„Ää„Ää„ÄäSubmission2(seresnext)„Äã„Äã„Äã\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c4a3c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:50.553389Z",
     "iopub.status.busy": "2025-06-05T22:18:50.553083Z",
     "iopub.status.idle": "2025-06-05T22:18:51.140027Z",
     "shell.execute_reply": "2025-06-05T22:18:51.138752Z"
    },
    "papermill": {
     "duration": 0.594853,
     "end_time": "2025-06-05T22:18:51.141892",
     "exception": false,
     "start_time": "2025-06-05T22:18:50.547039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33168a7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.154286Z",
     "iopub.status.busy": "2025-06-05T22:18:51.154008Z",
     "iopub.status.idle": "2025-06-05T22:18:51.159356Z",
     "shell.execute_reply": "2025-06-05T22:18:51.158457Z"
    },
    "papermill": {
     "duration": 0.013183,
     "end_time": "2025-06-05T22:18:51.161012",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.147829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52565e23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.173190Z",
     "iopub.status.busy": "2025-06-05T22:18:51.172902Z",
     "iopub.status.idle": "2025-06-05T22:18:51.185707Z",
     "shell.execute_reply": "2025-06-05T22:18:51.184474Z"
    },
    "papermill": {
     "duration": 0.02035,
     "end_time": "2025-06-05T22:18:51.187209",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.166859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06f3f5d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.200696Z",
     "iopub.status.busy": "2025-06-05T22:18:51.200416Z",
     "iopub.status.idle": "2025-06-05T22:18:51.214856Z",
     "shell.execute_reply": "2025-06-05T22:18:51.213769Z"
    },
    "papermill": {
     "duration": 0.023433,
     "end_time": "2025-06-05T22:18:51.216420",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.192987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f77d5e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.228676Z",
     "iopub.status.busy": "2025-06-05T22:18:51.228399Z",
     "iopub.status.idle": "2025-06-05T22:18:51.237151Z",
     "shell.execute_reply": "2025-06-05T22:18:51.235925Z"
    },
    "papermill": {
     "duration": 0.016514,
     "end_time": "2025-06-05T22:18:51.238665",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.222151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "395d5687",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.252012Z",
     "iopub.status.busy": "2025-06-05T22:18:51.251731Z",
     "iopub.status.idle": "2025-06-05T22:18:51.269423Z",
     "shell.execute_reply": "2025-06-05T22:18:51.268614Z"
    },
    "papermill": {
     "duration": 0.026357,
     "end_time": "2025-06-05T22:18:51.270812",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.244455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f849e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.283135Z",
     "iopub.status.busy": "2025-06-05T22:18:51.282836Z",
     "iopub.status.idle": "2025-06-05T22:18:51.290948Z",
     "shell.execute_reply": "2025-06-05T22:18:51.289719Z"
    },
    "papermill": {
     "duration": 0.015995,
     "end_time": "2025-06-05T22:18:51.292556",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.276561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26a251e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.305629Z",
     "iopub.status.busy": "2025-06-05T22:18:51.304772Z",
     "iopub.status.idle": "2025-06-05T22:18:51.314001Z",
     "shell.execute_reply": "2025-06-05T22:18:51.313231Z"
    },
    "papermill": {
     "duration": 0.017159,
     "end_time": "2025-06-05T22:18:51.315566",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.298407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f0599b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.327606Z",
     "iopub.status.busy": "2025-06-05T22:18:51.327301Z",
     "iopub.status.idle": "2025-06-05T22:18:51.338661Z",
     "shell.execute_reply": "2025-06-05T22:18:51.337499Z"
    },
    "papermill": {
     "duration": 0.018829,
     "end_time": "2025-06-05T22:18:51.340111",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.321282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bfd2898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:18:51.352185Z",
     "iopub.status.busy": "2025-06-05T22:18:51.351854Z",
     "iopub.status.idle": "2025-06-05T22:19:15.805498Z",
     "shell.execute_reply": "2025-06-05T22:19:15.804589Z"
    },
    "papermill": {
     "duration": 24.460838,
     "end_time": "2025-06-05T22:19:15.806583",
     "exception": false,
     "start_time": "2025-06-05T22:18:51.345745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bird2025-sed-ckpt/sedmodel.pth\n",
      "Model usage: Single model\n",
      "Found 10 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230420_164000\n",
      "Processing H02_20230420_223500\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Processing H02_20230421_190500\n",
      "Processing H02_20230421_233500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission0.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission0.csv\n",
      "Inference completed in 0.41 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "models = load_models(cfg, num_classes)\n",
    "\n",
    "if not models:\n",
    "    print(\"No models found! Please check model paths.\")\n",
    "    raise Exception(\"No models found! Please check model paths.\")\n",
    "\n",
    "print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "submission_path = 'submission0.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "smooth_submission(submission_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed74d9",
   "metadata": {
    "papermill": {
     "duration": 0.005183,
     "end_time": "2025-06-05T22:19:15.817683",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.812500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "„Ää„Ää„ÄäSubmission3(effieicntnet)„Äã„Äã„Äã\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be86d1f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.830020Z",
     "iopub.status.busy": "2025-06-05T22:19:15.829505Z",
     "iopub.status.idle": "2025-06-05T22:19:15.834749Z",
     "shell.execute_reply": "2025-06-05T22:19:15.833946Z"
    },
    "papermill": {
     "duration": 0.013016,
     "end_time": "2025-06-05T22:19:15.836165",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.823149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG_EfficientNet:\n",
    "    # ------------------------------------------- #\n",
    "    # [IMPORTANT]\n",
    "    # * Melspectrogram & Audio Params\n",
    "    # ------------------------------------------- #\n",
    "    N_FFT = 1034\n",
    "    HOP_LENGTH = 64\n",
    "    N_MELS = 136\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5\n",
    "\n",
    "    # ------------------------------------------- #\n",
    "    # * Model def\n",
    "    # ------------------------------------------- #\n",
    "    model_path = '/kaggle/input/4-efficientnet-b0-pytorch-train'\n",
    "    model_name = 'tf_efficientnetv2_s.in21k_ft_in1k'\n",
    "    use_specific_folds = True\n",
    "    folds = [1]\n",
    "    in_channels = 1\n",
    "    device = 'cpu'\n",
    "\n",
    "    # datasets\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False  \n",
    "    tta_count = 3\n",
    "    threshold = 0.7\n",
    "\n",
    "cfg_eff = CFG_EfficientNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cad393c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.848428Z",
     "iopub.status.busy": "2025-06-05T22:19:15.848142Z",
     "iopub.status.idle": "2025-06-05T22:19:15.856134Z",
     "shell.execute_reply": "2025-06-05T22:19:15.855273Z"
    },
    "papermill": {
     "duration": 0.015326,
     "end_time": "2025-06-05T22:19:15.857369",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.842043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c5ed3c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.870352Z",
     "iopub.status.busy": "2025-06-05T22:19:15.870045Z",
     "iopub.status.idle": "2025-06-05T22:19:15.875929Z",
     "shell.execute_reply": "2025-06-05T22:19:15.875064Z"
    },
    "papermill": {
     "duration": 0.013945,
     "end_time": "2025-06-05T22:19:15.877426",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.863481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModelEff(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,  \n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,    \n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        backbone_out = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1842a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.889895Z",
     "iopub.status.busy": "2025-06-05T22:19:15.889633Z",
     "iopub.status.idle": "2025-06-05T22:19:15.896278Z",
     "shell.execute_reply": "2025-06-05T22:19:15.895401Z"
    },
    "papermill": {
     "duration": 0.013869,
     "end_time": "2025-06-05T22:19:15.897390",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.883521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,\n",
    "        pad_mode=\"reflect\",\n",
    "        norm='slaney',\n",
    "        htk=True,\n",
    "        center=True,\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d131b5dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.910547Z",
     "iopub.status.busy": "2025-06-05T22:19:15.909727Z",
     "iopub.status.idle": "2025-06-05T22:19:15.922231Z",
     "shell.execute_reply": "2025-06-05T22:19:15.921194Z"
    },
    "papermill": {
     "duration": 0.020063,
     "end_time": "2025-06-05T22:19:15.923261",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.903198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_models_eff(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    model_files = find_model_files(cfg)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    if cfg.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in cfg.folds:\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            \n",
    "            model = BirdCLEFModelEff(cfg, num_classes)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram_eff(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "                \n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    if len(models) == 1:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)\n",
    "\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de556f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.935904Z",
     "iopub.status.busy": "2025-06-05T22:19:15.935598Z",
     "iopub.status.idle": "2025-06-05T22:19:15.943304Z",
     "shell.execute_reply": "2025-06-05T22:19:15.942096Z"
    },
    "papermill": {
     "duration": 0.015795,
     "end_time": "2025-06-05T22:19:15.944923",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.929128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_eff(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if debug:\n",
    "        test_files = []\n",
    "        for f in file_list:\n",
    "            test_files.append(f\"/kaggle/input/birdclef-2025/train_soundscapes/{f}.ogg\")\n",
    "        print(f\"Debug mode enabled.\")\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "            executor.map(\n",
    "                predict_on_spectrogram_eff,\n",
    "                test_files,\n",
    "                itertools.repeat(models),\n",
    "                itertools.repeat(cfg),\n",
    "                itertools.repeat(species_ids)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "\n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission_eff(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24ff3d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:15.958116Z",
     "iopub.status.busy": "2025-06-05T22:19:15.957869Z",
     "iopub.status.idle": "2025-06-05T22:19:41.149519Z",
     "shell.execute_reply": "2025-06-05T22:19:41.148045Z"
    },
    "papermill": {
     "duration": 25.199917,
     "end_time": "2025-06-05T22:19:41.150975",
     "exception": false,
     "start_time": "2025-06-05T22:19:15.951058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "Found a total of 5 model files.\n",
      "Using 1 model files for the specified folds ([1]).\n",
      "Loading model: /kaggle/input/4-efficientnet-b0-pytorch-train/model_fold1.pth\n",
      "Model usage: Single model\n",
      "Debug mode enabled.\n",
      "Found 8 test soundscapes\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Processing H02_20230421_190500\n",
      "Processing H02_20230421_233500\n",
      "Processing H02_20230422_014500\n",
      "Processing H02_20230422_021500\n",
      "Processing H02_20230422_024000\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission2.csv\n",
      "Inference completed in 0.42 minutes\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "start_time = time.time()\n",
    "print(\"Starting BirdCLEF-2025 inference...\")\n",
    "print(f\"TTA enabled: {cfg_eff.use_tta} (variations: {cfg_eff.tta_count if cfg_eff.use_tta else 0})\")\n",
    "\n",
    "models = load_models_eff(cfg_eff, num_classes)\n",
    "\n",
    "if not models:\n",
    "    raise Exception(\"No models found! Please check model paths.\")\n",
    "\n",
    "print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "row_ids, predictions = run_inference_eff(cfg_eff, models, species_ids)\n",
    "submission_df = create_submission_eff(row_ids, predictions, species_ids, cfg_eff)\n",
    "submission_path = 'submission2.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4689107e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:41.164122Z",
     "iopub.status.busy": "2025-06-05T22:19:41.163839Z",
     "iopub.status.idle": "2025-06-05T22:19:41.575067Z",
     "shell.execute_reply": "2025-06-05T22:19:41.573984Z"
    },
    "papermill": {
     "duration": 0.419403,
     "end_time": "2025-06-05T22:19:41.576629",
     "exception": false,
     "start_time": "2025-06-05T22:19:41.157226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# post_processing\n",
    "sub = pd.read_csv('submission2.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce586d",
   "metadata": {
    "papermill": {
     "duration": 0.005567,
     "end_time": "2025-06-05T22:19:41.588492",
     "exception": false,
     "start_time": "2025-06-05T22:19:41.582925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "„Ää„Ää„ÄäFinaly Blending„Äã„Äã„Äã\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34ee3814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:41.671607Z",
     "iopub.status.busy": "2025-06-05T22:19:41.671257Z",
     "iopub.status.idle": "2025-06-05T22:19:41.675487Z",
     "shell.execute_reply": "2025-06-05T22:19:41.674524Z"
    },
    "papermill": {
     "duration": 0.013151,
     "end_time": "2025-06-05T22:19:41.677191",
     "exception": false,
     "start_time": "2025-06-05T22:19:41.664040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------- #\n",
    "sub_w=[0.5, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f02d5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T22:19:41.691479Z",
     "iopub.status.busy": "2025-06-05T22:19:41.691175Z",
     "iopub.status.idle": "2025-06-05T22:19:45.231532Z",
     "shell.execute_reply": "2025-06-05T22:19:45.230242Z"
    },
    "papermill": {
     "duration": 3.548579,
     "end_time": "2025-06-05T22:19:45.233020",
     "exception": false,
     "start_time": "2025-06-05T22:19:41.684441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_TARGETs = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "list_targets_0 = [f'{TARGET} 0' for TARGET in list_TARGETs]\n",
    "list_targets_1 = [f'{TARGET} 1' for TARGET in list_TARGETs]\n",
    "list_targets_2 = [f'{TARGET} 2' for TARGET in list_TARGETs]\n",
    "\n",
    "df0 = pd.read_csv(\"/kaggle/working/submission0.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "df2 = pd.read_csv(\"/kaggle/working/submission2.csv\")\n",
    "\n",
    "df0 = df0.rename(columns={TARGET : f'{TARGET} 0' for TARGET in list_TARGETs})\n",
    "df1 = df1.rename(columns={TARGET : f'{TARGET} 1' for TARGET in list_TARGETs})\n",
    "df2 = df2.rename(columns={TARGET : f'{TARGET} 2' for TARGET in list_TARGETs})\n",
    "\n",
    "dfs = pd.merge(df0,df1,on=['row_id'])\n",
    "dfs = pd.merge(dfs,df2,on=['row_id'])\n",
    "\n",
    "for i in range(len(list_TARGETs)):\n",
    "    dfs[list_TARGETs[i]] = dfs[list_targets_0[i]]*sub_w[0] + sub_w[1]*dfs[list_targets_1[i]] + sub_w[2]*dfs[list_targets_2[i]]\n",
    "             \n",
    "for col0,col1,col2 in zip(list_targets_0, list_targets_1, list_targets_2):\n",
    "    del dfs[col0]\n",
    "    del dfs[col1]\n",
    "    del dfs[col2]\n",
    "    \n",
    "    \n",
    "dfs.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8b7ed6",
   "metadata": {
    "papermill": {
     "duration": 0.006518,
     "end_time": "2025-06-05T22:19:45.246102",
     "exception": false,
     "start_time": "2025-06-05T22:19:45.239584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 242436130,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 103.111446,
   "end_time": "2025-06-05T22:19:48.811376",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T22:18:05.699930",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
